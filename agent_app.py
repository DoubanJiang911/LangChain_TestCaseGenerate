import re
from io import BytesIO
from typing import List

import pandas as pd
import streamlit as st
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field, ValidationError
from pypdf import PdfReader
import docx


# ---------------------------------------------------------
# Data models
# ---------------------------------------------------------
class FeatureItem(BaseModel):
    module: str = Field(description="æ‰€å±æ¨¡å—æˆ–ä¸šåŠ¡åŸŸ")
    feature: str = Field(description="åŠŸèƒ½åç§°")
    description: str = Field(default="", description="éœ€æ±‚æ‘˜è¦")
    acceptance: List[str] = Field(default_factory=list, description="å…³é”®éªŒæ”¶ç‚¹æˆ–è§„åˆ™")
    dependencies: List[str] = Field(default_factory=list, description="ä¾èµ–/å‰æ")


class FeatureCollection(BaseModel):
    features: List[FeatureItem]


class TestCase(BaseModel):
    case_id: str
    module: str
    feature: str
    title: str
    precondition: str
    steps: str
    expected: str
    priority: str
    type: str


class TestSuite(BaseModel):
    cases: List[TestCase]


# ---------------------------------------------------------
# File helpers
# ---------------------------------------------------------
def extract_text(uploaded_file):
    ext = uploaded_file.name.split(".")[-1].lower()
    text = ""

    if ext == "pdf":
        reader = PdfReader(uploaded_file)
        for page in reader.pages:
            text += (page.extract_text() or "") + "\n"
    elif ext == "docx":
        document = docx.Document(uploaded_file)
        for para in document.paragraphs:
            text += para.text + "\n"
    elif ext in {"txt", "md"}:
        text = uploaded_file.read().decode("utf-8")
    else:
        raise ValueError("ä»…æ”¯æŒ PDF / DOCX / TXT / MD æ–‡ä»¶")

    cleaned = re.sub(r"\n{3,}", "\n\n", text).strip()
    if not cleaned:
        raise ValueError("æœªèƒ½ä»æ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼Œè¯·ç¡®è®¤æ–‡ä»¶å†…å®¹ã€‚")
    return cleaned


def chunk_text(text, chunk_size=1800, overlap=200):
    words = text.split()
    if len(words) <= chunk_size:
        return [text]

    chunks = []
    start = 0
    while start < len(words):
        end = min(len(words), start + chunk_size)
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return chunks


def df_to_excel_bytes(df: pd.DataFrame, sheet_name: str):
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet_name)
    buffer.seek(0)
    return buffer.getvalue()


# åˆå§‹åŒ– session state
for key in ["feature_df", "feature_bytes", "case_df", "case_bytes"]:
    st.session_state.setdefault(key, None)


# ---------------------------------------------------------
# Agent steps
# ---------------------------------------------------------
def init_llm(api_key, base_url, model_name, temperature):
    if not api_key:
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
        return None
    return ChatOpenAI(
        api_key=api_key,
        base_url=base_url or None,
        model=model_name,
        temperature=temperature,
        max_retries=2,
    )


def _normalize_feature_response(result):
    """
    å°†ä¸åŒå½¢æ€çš„æ¨¡å‹è¿”å›å€¼ç»Ÿä¸€è½¬æ¢ä¸º FeatureItem åˆ—è¡¨ã€‚
    """
    if result is None:
        return []

    raw_items = []
    if hasattr(result, "features"):
        raw_items = getattr(result, "features", [])
    elif isinstance(result, dict):
        raw_items = result.get("features", [])
    elif isinstance(result, list):
        raw_items = result
    else:
        raw_items = []

    normalized = []
    for item in raw_items:
        if isinstance(item, FeatureItem):
            normalized.append(item)
        elif isinstance(item, dict):
            try:
                normalized.append(FeatureItem(**item))
            except ValidationError:
                continue
    return normalized


def analyze_features(llm, text):
    parser = JsonOutputParser(pydantic_object=FeatureCollection)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯èµ„æ·±éœ€æ±‚åˆ†æå¸ˆï¼Œè¯·ä»éœ€æ±‚ç‰‡æ®µä¸­æå–åŠŸèƒ½æ¨¡å—ä¸åŠŸèƒ½é¡¹ã€‚"
                   "è¦æ±‚èšç„¦ä¸šåŠ¡ç›®æ ‡ï¼Œè¡¥å……å…³é”®éªŒæ”¶ç‚¹/æ ¡éªŒè§„åˆ™ã€‚"
                   "\n{format_instructions}"),
        ("human", "éœ€æ±‚ç‰‡æ®µï¼ˆID: {segment_id}ï¼‰ï¼š\n{segment_text}")
    ])

    chain = prompt | llm | parser
    segments = chunk_text(text)
    collected = []

    for idx, seg in enumerate(segments, start=1):
        with st.spinner(f"åˆ†æåŠŸèƒ½ç‰‡æ®µ {idx}/{len(segments)}"):
            try:
                result = chain.invoke({
                    "segment_id": f"seg_{idx}",
                    "segment_text": seg,
                    "format_instructions": parser.get_format_instructions()
                })
                collected.extend(_normalize_feature_response(result))
            except Exception as err:
                st.warning(f"ç‰‡æ®µ {idx} æå–å¤±è´¥ï¼š{err}")

    # å»é‡ï¼šmodule + feature ä½œä¸º key
    unique = {}
    for feature in collected:
        key = (feature.module.strip(), feature.feature.strip())
        if key not in unique:
            unique[key] = feature
    return list(unique.values())


def estimate_case_target(feature: FeatureItem, max_cases: int):
    """
    æ ¹æ®åŠŸèƒ½å¤æ‚åº¦ä¼°ç®—éœ€è¦çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡ã€‚
    ç®€å•å¯å‘å¼ï¼šåŸºäºæè¿°é•¿åº¦ã€éªŒæ”¶ç‚¹æ•°ä»¥åŠä¾èµ–ä¸ªæ•°ã€‚
    """
    base = 1
    desc_len = len(feature.description.split())
    acceptance_count = len(feature.acceptance)
    dependency_count = len(feature.dependencies)

    if desc_len > 80:
        base += 2
    elif desc_len > 40:
        base += 1

    base += min(acceptance_count, 3)
    base += min(dependency_count, 2)

    return max(1, min(max_cases, base))


def generate_cases(llm, features, max_cases):
    parser = JsonOutputParser(pydantic_object=TestSuite)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯æµ‹è¯•æ¶æ„å¸ˆï¼Œæ ¹æ®åŠŸèƒ½å®šä¹‰è®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–æ­£å‘ã€å¼‚å¸¸ã€è¾¹ç•Œã€‚"
                   "æ¯æ¡ç”¨ä¾‹éœ€è¦ case_id/module/feature/title/precondition/"
                   "steps/expected/priority/typeã€‚ä½ åº”æ ¹æ®åŠŸèƒ½å¤æ‚åº¦ï¼Œç”Ÿæˆåˆé€‚æ•°é‡çš„ç”¨ä¾‹ï¼Œ"
                   "ä½†ä¸è¦å°‘äº 1 æ¡ï¼Œä¹Ÿä¸è¦è¶…è¿‡ç”¨æˆ·æŒ‡å®šçš„ä¸Šé™ã€‚\n{format_instructions}"),
        ("human", "åŠŸèƒ½ä¿¡æ¯ï¼š\n{feature_payload}\n"
                  "è¯·æ ¹æ®å¤æ‚åº¦ç”Ÿæˆ {target_cases}~{max_cases} æ¡ä»£è¡¨æ€§ç”¨ä¾‹ï¼Œ"
                  "è‹¥åŠŸèƒ½ç®€å•å¯è¾“å‡ºæ›´å°‘ï¼Œä½†è‡³å°‘ 1 æ¡ã€‚")
    ])

    chain = prompt | llm | parser
    all_cases = []

    def _normalize_case_response(result, fallback_module, fallback_feature):
        if result is None:
            return []

        raw_cases = []
        if hasattr(result, "cases"):
            raw_cases = getattr(result, "cases", [])
        elif isinstance(result, dict):
            raw_cases = result.get("cases", [])
        elif isinstance(result, list):
            raw_cases = result

        normalized = []
        for case in raw_cases:
            if isinstance(case, TestCase):
                data = case.model_dump()
            elif isinstance(case, dict):
                try:
                    data = TestCase(**case).model_dump()
                except ValidationError:
                    continue
            else:
                continue

            # ç¡®ä¿ module/feature å¡«å……
            data.setdefault("module", fallback_module)
            data.setdefault("feature", fallback_feature)
            normalized.append(data)
        return normalized

    for idx, feature in enumerate(features, start=1):
        payload = feature.model_dump()
        try:
            with st.spinner(f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ {idx}/{len(features)}"):
                target_cases = estimate_case_target(feature, max_cases)
                result = chain.invoke({
                    "feature_payload": payload,
                    "target_cases": target_cases,
                    "max_cases": max_cases,
                    "format_instructions": parser.get_format_instructions()
                })
                all_cases.extend(
                    _normalize_case_response(result, feature.module, feature.feature)
                )
        except Exception as err:
            st.warning(f"åŠŸèƒ½ã€Œ{feature.feature}ã€ç”Ÿæˆå¤±è´¥ï¼š{err}")

    return all_cases


# ---------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------
st.set_page_config(page_title="Agent Â· éœ€æ±‚åˆ°æµ‹è¯•ç”¨ä¾‹", layout="wide")
st.title("ğŸ¤– Agent æµç¨‹ï¼šéœ€æ±‚ç†è§£ âœ åŠŸèƒ½æ¢³ç† âœ æµ‹è¯•ç”¨ä¾‹")

with st.sidebar:
    st.header("LLM é…ç½®")
    api_key = st.text_input("API Key", type="password")
    base_url = st.text_input("Base URL (å¯é€‰)")
    model_name = st.selectbox(
        "æ¨¡å‹",
        ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "deepseek-chat"],
        index=0
    )
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
    max_cases = st.slider("æ¯ä¸ªåŠŸèƒ½çš„æœ€å¤§ç”¨ä¾‹æ•°", 2, 36, 3)

    st.markdown("---")
    st.caption("æ”¯æŒä¸Šä¼  PDF / DOCX / TXTï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç²˜è´´éœ€æ±‚æ–‡æœ¬ã€‚")


uploaded = st.file_uploader("ä¸Šä¼ éœ€æ±‚æ–‡æ¡£ (PDF/DOCX/TXT/MD)", type=["pdf", "docx", "txt", "md"])
text_input = st.text_area("æˆ–ç›´æ¥ç²˜è´´éœ€æ±‚å†…å®¹", height=200)

document_text = ""
if uploaded:
    try:
        document_text = extract_text(uploaded)
        st.success(f"æ–‡æ¡£è§£ææˆåŠŸï¼Œå­—ç¬¦æ•°ï¼š{len(document_text)}")
        with st.expander("æŸ¥çœ‹æ–‡æ¡£å†…å®¹"):
            st.text(document_text[:4000] + ("..." if len(document_text) > 4000 else ""))
    except Exception as exc:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")

elif text_input.strip():
    document_text = text_input.strip()


def render_feature_results():
    df = st.session_state.get("feature_df")
    data = st.session_state.get("feature_bytes")
    if df is None:
        return
    st.subheader("æ­¥éª¤ 1ï¼šåŠŸèƒ½æ¢³ç†")
    st.dataframe(df, use_container_width=True)
    if data:
        st.download_button(
            "ğŸ“¥ ä¸‹è½½åŠŸèƒ½æ¸…å•",
            data=data,
            file_name="features.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="download_features_btn"
        )


def render_case_results():
    df = st.session_state.get("case_df")
    data = st.session_state.get("case_bytes")
    if df is None:
        return
    st.subheader("æ­¥éª¤ 2ï¼šè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    st.dataframe(df, use_container_width=True)
    if data:
        st.download_button(
            "ğŸ“¥ ä¸‹è½½æµ‹è¯•ç”¨ä¾‹",
            data=data,
            file_name="test_cases.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="download_cases_btn"
        )


if st.button("ğŸš€ è¿è¡Œ Agent æµç¨‹", type="primary"):
    st.session_state["feature_df"] = None
    st.session_state["feature_bytes"] = None
    st.session_state["case_df"] = None
    st.session_state["case_bytes"] = None

    if not document_text:
        st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–ç²˜è´´éœ€æ±‚å†…å®¹ã€‚")
    else:
        llm = init_llm(api_key, base_url, model_name, temperature)
        if llm:
            features = analyze_features(llm, document_text)

            if not features:
                st.error("æ²¡æœ‰æå–åˆ°åŠŸèƒ½ç‚¹ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹ã€‚")
            else:
                feature_df = pd.DataFrame([f.dict() for f in features])
                st.session_state["feature_df"] = feature_df
                st.session_state["feature_bytes"] = df_to_excel_bytes(feature_df, "Features")

                cases = generate_cases(llm, features, max_cases)

                if cases:
                    case_df = pd.DataFrame(cases)
                    st.session_state["case_df"] = case_df
                    st.session_state["case_bytes"] = df_to_excel_bytes(case_df, "TestCases")
                else:
                    st.warning("LLM æ²¡æœ‰è¿”å›æµ‹è¯•ç”¨ä¾‹ï¼Œè¯·å°è¯•å‡å°‘æ–‡æ¡£é•¿åº¦æˆ–æ›´æ¢æ¨¡å‹ã€‚")


render_feature_results()
render_case_results()

