import re
from io import BytesIO
import json
from typing import List
from datetime import datetime

import pandas as pd
import streamlit as st
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field, ValidationError
from pypdf import PdfReader
import docx


# ---------------------------------------------------------
# Data models
# ---------------------------------------------------------
class FeatureItem(BaseModel):
    module: str = Field(description="æ‰€å±æ¨¡å—æˆ–ä¸šåŠ¡åŸŸ")
    feature: str = Field(description="åŠŸèƒ½åç§°")
    description: str = Field(default="", description="éœ€æ±‚æ‘˜è¦")
    acceptance: List[str] = Field(default_factory=list, description="å…³é”®éªŒæ”¶ç‚¹æˆ–è§„åˆ™")
    dependencies: List[str] = Field(default_factory=list, description="ä¾èµ–/å‰æ")


class FeatureCollection(BaseModel):
    features: List[FeatureItem]


class TestCase(BaseModel):
    case_id: str
    module: str
    feature: str
    title: str
    precondition: str
    steps: str
    expected: str
    priority: str
    type: str


class TestSuite(BaseModel):
    cases: List[TestCase]


# ---------------------------------------------------------
# File helpers
# ---------------------------------------------------------
def extract_text(uploaded_file):
    ext = uploaded_file.name.split(".")[-1].lower()
    text = ""

    if ext == "pdf":
        reader = PdfReader(uploaded_file)
        for page in reader.pages:
            text += (page.extract_text() or "") + "\n"
    elif ext == "docx":
        document = docx.Document(uploaded_file)
        for para in document.paragraphs:
            text += para.text + "\n"
    elif ext in {"txt", "md"}:
        text = uploaded_file.read().decode("utf-8")
    else:
        raise ValueError("ä»…æ”¯æŒ PDF / DOCX / TXT / MD æ–‡ä»¶")

    cleaned = re.sub(r"\n{3,}", "\n\n", text).strip()
    if not cleaned:
        raise ValueError("æœªèƒ½ä»æ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼Œè¯·ç¡®è®¤æ–‡ä»¶å†…å®¹ã€‚")
    return cleaned


def chunk_text(text, chunk_size=1800, overlap=200):
    words = text.split()
    if len(words) <= chunk_size:
        return [text]

    chunks = []
    start = 0
    while start < len(words):
        end = min(len(words), start + chunk_size)
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return chunks


# ---------------------------------------------------------
# Agent steps
# ---------------------------------------------------------
def init_llm(api_key, base_url, model_name, temperature):
    if not api_key:
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
        return None
    return ChatOpenAI(
        api_key=api_key,
        base_url=base_url or None,
        model=model_name,
        temperature=temperature,
        max_retries=2,
    )


def _normalize_feature_response(result):
    """
    å°†ä¸åŒå½¢æ€çš„æ¨¡å‹è¿”å›å€¼ç»Ÿä¸€è½¬æ¢ä¸º FeatureItem åˆ—è¡¨ã€‚
    """
    if result is None:
        return []

    raw_items = []
    if hasattr(result, "features"):
        raw_items = getattr(result, "features", [])
    elif isinstance(result, dict):
        raw_items = result.get("features", [])
    elif isinstance(result, list):
        raw_items = result
    else:
        raw_items = []

    normalized = []
    for item in raw_items:
        if isinstance(item, FeatureItem):
            normalized.append(item)
        elif isinstance(item, dict):
            try:
                normalized.append(FeatureItem(**item))
            except ValidationError:
                continue
    return normalized


def analyze_features(llm, text, debug: bool = False, log_fn=None):
    parser = JsonOutputParser(pydantic_object=FeatureCollection)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯èµ„æ·±éœ€æ±‚åˆ†æå¸ˆï¼Œè¯·ä»éœ€æ±‚ç‰‡æ®µä¸­æå–åŠŸèƒ½æ¨¡å—ä¸åŠŸèƒ½é¡¹ã€‚"
                   "è¦æ±‚èšç„¦ä¸šåŠ¡ç›®æ ‡ï¼Œè¡¥å……å…³é”®éªŒæ”¶ç‚¹/æ ¡éªŒè§„åˆ™ã€‚"
                   "\n{format_instructions}"),
        ("human", "éœ€æ±‚ç‰‡æ®µï¼ˆID: {segment_id}ï¼‰ï¼š\n{segment_text}")
    ])

    chain = prompt | llm | parser
    segments = chunk_text(text)
    collected = []

    for idx, seg in enumerate(segments, start=1):
        with st.spinner(f"åˆ†æåŠŸèƒ½ç‰‡æ®µ {idx}/{len(segments)}"):
            try:
                input_payload = {
                    "segment_id": f"seg_{idx}",
                    "segment_text": seg,
                    "format_instructions": parser.get_format_instructions()
                }
                if debug and log_fn:
                    log_fn({
                        "call": "analyze_features",
                        "phase": "input",
                        "segment_id": input_payload.get("segment_id"),
                        "payload": input_payload
                    })

                result = chain.invoke(input_payload)

                if debug and log_fn:
                    out_value = result if not hasattr(result, 'model_dump') else result.model_dump()
                    log_fn({
                        "call": "analyze_features",
                        "phase": "output",
                        "segment_id": input_payload.get("segment_id"),
                        "payload": out_value
                    })

                collected.extend(_normalize_feature_response(result))
            except Exception as err:
                st.warning(f"ç‰‡æ®µ {idx} æå–å¤±è´¥ï¼š{err}")

    # å»é‡ï¼šmodule + feature ä½œä¸º key
    unique = {}
    for feature in collected:
        key = (feature.module.strip(), feature.feature.strip())
        if key not in unique:
            unique[key] = feature
    return list(unique.values())


def generate_cases(llm, features, max_cases, debug: bool = False, log_fn=None):
    parser = JsonOutputParser(pydantic_object=TestSuite)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯æµ‹è¯•æ¶æ„å¸ˆï¼Œæ ¹æ®åŠŸèƒ½å®šä¹‰è®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–æ­£å‘ã€å¼‚å¸¸ã€è¾¹ç•Œã€‚"
                   "æ¯æ¡ç”¨ä¾‹éœ€è¦ case_id/module/feature/title/precondition/"
                   "steps/expected/priority/typeã€‚\n{format_instructions}"),
        ("human", "åŠŸèƒ½ä¿¡æ¯ï¼š\n{feature_payload}\n"
                  "è¯·è¾“å‡ºä¸è¶…è¿‡ {max_cases} æ¡ä»£è¡¨æ€§æµ‹è¯•ç”¨ä¾‹ã€‚")
    ])

    chain = prompt | llm | parser
    all_cases = []

    def _normalize_case_response(result, fallback_module, fallback_feature):
        if result is None:
            return []

        raw_cases = []
        if hasattr(result, "cases"):
            raw_cases = getattr(result, "cases", [])
        elif isinstance(result, dict):
            raw_cases = result.get("cases", [])
        elif isinstance(result, list):
            raw_cases = result

        normalized = []
        for case in raw_cases:
            if isinstance(case, TestCase):
                data = case.model_dump()
            elif isinstance(case, dict):
                try:
                    data = TestCase(**case).model_dump()
                except ValidationError:
                    continue
            else:
                continue

            # ç¡®ä¿ module/feature å¡«å……
            data.setdefault("module", fallback_module)
            data.setdefault("feature", fallback_feature)
            normalized.append(data)
        return normalized

    for idx, feature in enumerate(features, start=1):
        payload = feature.model_dump()
        try:
            with st.spinner(f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ {idx}/{len(features)}"):
                input_payload = {
                    "feature_payload": payload,
                    "max_cases": max_cases,
                    "format_instructions": parser.get_format_instructions()
                }
                if debug and log_fn:
                    log_fn({
                        "call": "generate_cases",
                        "phase": "input",
                        "feature": payload.get("module"),
                        "payload": input_payload
                    })

                result = chain.invoke(input_payload)

                if debug and log_fn:
                    out_value = result if not hasattr(result, 'model_dump') else result.model_dump()
                    log_fn({
                        "call": "generate_cases",
                        "phase": "output",
                        "feature": payload.get("module"),
                        "payload": out_value
                    })

                all_cases.extend(
                    _normalize_case_response(result, feature.module, feature.feature)
                )
        except Exception as err:
            st.warning(f"åŠŸèƒ½ã€Œ{feature.feature}ã€ç”Ÿæˆå¤±è´¥ï¼š{err}")

    return all_cases


# ---------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------
st.set_page_config(page_title="Agent Â· éœ€æ±‚åˆ°æµ‹è¯•ç”¨ä¾‹", layout="wide")
st.title("ğŸ¤– Agent æµç¨‹ï¼šéœ€æ±‚ç†è§£ âœ åŠŸèƒ½æ¢³ç† âœ æµ‹è¯•ç”¨ä¾‹")

with st.sidebar:
    st.header("LLM é…ç½®")
    api_key = st.text_input("API Key", type="password")
    base_url = st.text_input("Base URL (å¯é€‰)")
    model_name = st.selectbox(
        "æ¨¡å‹",
        ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "deepseek-chat"],
        index=0
    )
    temperature = st.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
    max_cases = st.slider("æ¯ä¸ªåŠŸèƒ½çš„æœ€å¤§ç”¨ä¾‹æ•°", 2, 36, 3)

    st.markdown("---")
    st.caption("æ”¯æŒä¸Šä¼  PDF / DOCX / TXTï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç²˜è´´éœ€æ±‚æ–‡æœ¬ã€‚")
    debug = st.checkbox("è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºæ¯æ¬¡ LLM äº¤äº’çš„è¾“å…¥/è¾“å‡º", key="debug")


# æ ¹æ®è°ƒè¯•å¼€å…³æ‹†åˆ†ä¸»è§†å›¾ï¼šå‹¾é€‰æ—¶ä¸ºä¸¤æ å¸ƒå±€ï¼ˆå·¦ï¼šä¸»æµç¨‹ï¼Œå³ï¼šå›ºå®šè°ƒè¯•æ—¥å¿—ï¼‰ï¼›æœªå‹¾é€‰æ—¶ä¸æ˜¾ç¤ºè°ƒè¯•åŒºåŸŸ
if debug:
    left_col, right_col = st.columns([1, 1])
    main = left_col
    right_col.markdown("### è°ƒè¯•è¾“å‡º")
    # æ¸…ç©ºæ—¥å¿—æŒ‰é’®
    if right_col.button("æ¸…ç©ºæ—¥å¿—", key="clear_logs"):
        st.session_state['debug_logs'] = []

    # ä¸‹è½½æ—¥å¿— (.log)
    def _build_log_text():
        logs = st.session_state.get('debug_logs', [])
        sep = "\n" + ("-" * 60) + "\n"
        parts = []
        for e in logs:
            try:
                parts.append(sep + json.dumps(e, ensure_ascii=False, indent=2) + sep)
            except Exception:
                parts.append(sep + repr(e) + sep)
        return "\n".join(parts)

    right_col.download_button(
        "ä¸‹è½½æ—¥å¿— (.log)",
        data=_build_log_text().encode("utf-8"),
        file_name="llm_debug.log",
        mime="text/plain",
        key="download_logs",
    )

    # debug_parent ç”¨äºæ¸²æŸ“æ›´å¤æ‚çš„ debug UIï¼ˆåˆ†ç»„ expander åˆ—è¡¨ï¼‰
    debug_parent = right_col
else:
    main = st
    debug_parent = None

uploaded = main.file_uploader("ä¸Šä¼ éœ€æ±‚æ–‡æ¡£ (PDF/DOCX/TXT/MD)", type=["pdf", "docx", "txt", "md"])
text_input = main.text_area("æˆ–ç›´æ¥ç²˜è´´éœ€æ±‚å†…å®¹", height=200)

# è°ƒè¯•è¾“å‡ºæ•°æ®å­˜å‚¨ï¼ˆå§‹ç»ˆä¿ç•™ï¼‰ï¼Œä½†ä»…åœ¨ debug=True æ—¶æ¸²æŸ“
if 'debug_logs' not in st.session_state:
    st.session_state['debug_logs'] = []

def render_debug_ui(parent):
    """åœ¨ç»™å®šçš„åˆ—/å®¹å™¨ä¸­æ¸²æŸ“æŒ‰è°ƒç”¨ç±»å‹åˆ†ç»„çš„æ—¥å¿—ï¼ˆæœ€æ–°åœ¨å‰ï¼‰ã€‚

    parent: DeltaGeneratorï¼ˆä¾‹å¦‚å³ä¾§åˆ—ï¼‰
    """
    if parent is None:
        return

    # æ–°å»ºä¸€ä¸ªå ä½åŒºåŸŸæ¥é‡æ–°æ¸²æŸ“
    display = parent.container()
    logs = list(reversed(st.session_state.get('debug_logs', [])))  # æœ€æ–°åœ¨å‰

    # æŒ‰è°ƒç”¨ç±»å‹åˆ†ç»„
    groups = {}
    for idx, entry in enumerate(logs):
        call = entry.get('call', 'other') if isinstance(entry, dict) else 'other'
        groups.setdefault(call, []).append((idx, entry))

    for call, items in groups.items():
        display.subheader(f"{call} ({len(items)})")
        for i, entry in items:
            header = f"{entry.get('ts','')} â€” {entry.get('phase', '')}"
            with display.expander(header, expanded=False):
                try:
                    display.json(entry)
                except Exception:
                    display.text(repr(entry))


def append_debug(entry):
    """è¿½åŠ ä¸€æ¡ç»“æ„åŒ–æ—¥å¿—åˆ° `st.session_state['debug_logs']` å¹¶åœ¨ debug æ—¶ç”¨ `st.json` æ¸²æŸ“æ˜¾ç¤ºã€‚

    entry å¯ä»¥æ˜¯ dictï¼ˆæ¨èï¼‰æˆ–ä»»æ„å¯åºåˆ—åŒ–å¯¹è±¡ã€‚
    """
    try:
        logs = st.session_state.setdefault('debug_logs', [])
        ts = datetime.utcnow().isoformat() + "Z"
        if isinstance(entry, dict):
            entry_obj = {"ts": ts, **entry}
        else:
            entry_obj = {"ts": ts, "message": entry}

        logs.append(entry_obj)
        if len(logs) > 200:
            st.session_state['debug_logs'] = logs[-200:]

        # ä»…åœ¨ debug æ—¶æ›´æ–°å³ä¾§æ˜¾ç¤ºï¼ˆç”¨åˆ†ç»„ expander åˆ—è¡¨ï¼‰
        if debug and debug_parent is not None:
            try:
                render_debug_ui(debug_parent)
            except Exception:
                try:
                    debug_parent.text("\n".join([repr(x) for x in st.session_state['debug_logs']]))
                except Exception:
                    pass
    except Exception:
        if debug and debug_parent is not None:
            try:
                debug_parent.text(repr(entry))
            except Exception:
                pass

# é¡µé¢åˆå§‹æ¸²æŸ“ï¼šå¦‚æœå·²å¼€å¯ debugï¼Œåˆ™æ¸²æŸ“ç°æœ‰æ—¥å¿—
if debug and debug_parent is not None:
    try:
        render_debug_ui(debug_parent)
    except Exception:
        try:
            debug_parent.text("\n".join([repr(x) for x in st.session_state.get('debug_logs', [])]))
        except Exception:
            pass

document_text = ""
if uploaded:
    try:
        document_text = extract_text(uploaded)
        main.success(f"æ–‡æ¡£è§£ææˆåŠŸï¼Œå­—ç¬¦æ•°ï¼š{len(document_text)}")
        with main.expander("æŸ¥çœ‹æ–‡æ¡£å†…å®¹"):
            main.text(document_text[:4000] + ("..." if len(document_text) > 4000 else ""))
    except Exception as exc:
        main.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")

elif text_input.strip():
    document_text = text_input.strip()


if main.button("ğŸš€ è¿è¡Œ Agent æµç¨‹", type="primary"):
    if not document_text:
        main.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–ç²˜è´´éœ€æ±‚å†…å®¹ã€‚")
    else:
        llm = init_llm(api_key, base_url, model_name, temperature)
        if llm:
            main.subheader("æ­¥éª¤ 1ï¼šåŠŸèƒ½æ¢³ç†")
            features = analyze_features(llm, document_text, debug=debug, log_fn=append_debug)

            if not features:
                main.error("æ²¡æœ‰æå–åˆ°åŠŸèƒ½ç‚¹ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹ã€‚")
            else:
                feature_df = pd.DataFrame([f.dict() for f in features])
                main.dataframe(feature_df, use_container_width=True)

                feature_out = BytesIO()
                with pd.ExcelWriter(feature_out, engine="xlsxwriter") as writer:
                    feature_df.to_excel(writer, index=False, sheet_name="Features")

                main.download_button(
                    "ğŸ“¥ ä¸‹è½½åŠŸèƒ½æ¸…å•",
                    feature_out.getvalue(),
                    "features.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

                main.subheader("æ­¥éª¤ 2ï¼šè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
                cases = generate_cases(llm, features, max_cases, debug=debug, log_fn=append_debug)

                if cases:
                    case_df = pd.DataFrame(cases)
                    main.dataframe(case_df, use_container_width=True)

                    case_out = BytesIO()
                    with pd.ExcelWriter(case_out, engine="xlsxwriter") as writer:
                        case_df.to_excel(writer, index=False, sheet_name="TestCases")

                    main.download_button(
                        "ğŸ“¥ ä¸‹è½½æµ‹è¯•ç”¨ä¾‹",
                        case_out.getvalue(),
                        "test_cases.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                else:
                    main.warning("LLM æ²¡æœ‰è¿”å›æµ‹è¯•ç”¨ä¾‹ï¼Œè¯·å°è¯•å‡å°‘æ–‡æ¡£é•¿åº¦æˆ–æ›´æ¢æ¨¡å‹ã€‚")

